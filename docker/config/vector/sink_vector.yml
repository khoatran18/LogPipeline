sources:
  source_kafka:
    type: kafka
    bootstrap_servers: broker1:29092
    group_id: sink_topic
    topics:
      - sink_topic
    auto_offset_reset: earliest
    acknowledgements:
      enabled: true
    decoding:
      codec: json
    drain_timeout_ms: 2500
    fetch_wait_max_ms: 100

transforms:
  del_sub_fields:
    type: remap
    inputs:
      - source_kafka
    source: |
      del(.message_key)
      del(.offset)
      del(.partition)
      del(.topic)
      del(.source_type)
      del(.headers)

  s3_route:
    type: remap
    inputs:
      - del_sub_fields
    source: |
      true
      

  clickhouse_route:
    type: remap
    inputs:
      - del_sub_fields
    source: |
      . = flatten!(., "_")
      .timestamp = parse_timestamp!(.timestamp, "%+")
      .timestamp = format_timestamp!(.timestamp, "%Y-%m-%d %H:%M:%S")

  elasticsearch_route:
    type: remap
    inputs:
      - del_sub_fields
    source: |
      #for_each(keys(.)) -> |_, key| {
      #  if (contains(key, "geo")) {
      #    value = get!(., [key])
      #    point_name = key + "_point"
      #    country_name = key + "_country"
      #    point = {
      #      "lat": value.latitude,
      #      "lon": value.longitude
      #    }
      #    country = value.country
      #    . = set!(., [point_name], point)
      #   . =  set!(., [country_name], country)
      #    #del(.[key])
      #  }
      #}
      true
      
      

sinks:

  console_output:
    type: console
    inputs:
      - clickhouse_route
    target: stdout
    encoding:
      codec: json

  s3_sink:
    type: aws_s3
    inputs:
      - s3_route
    endpoint: http://minio:9000
    region: us-east-1
    bucket: log-bucket
    key_prefix: "year=%Y/month=%m/day=%d/{{ log_type }}/"
    force_path_style: true
    auth:
      access_key_id: minio
      secret_access_key: minio123
    buffer:
      type: disk
      max_size: 268435490
      when_full: block
    compression: gzip
    content_encoding: gzip
    filename_extension: gz
    content_type: application/json
    batch:
      max_events: 10
      max_timeout_secs: 10
    encoding:
      codec: json



  elasticsearch_sink:
    type: elasticsearch
    inputs:
      - elasticsearch_route
    endpoint: http://elasticsearch:9200
    api_version: auto
    batch:
      max_events: 10
      timeout_secs: 10
    buffer:
      type: disk
      max_size: 268435488
      when_full: block
    mode: bulk
    bulk:
      action: index
      index: "{{ log_type }}"
      template_fallback_index: "default_log"
    distribution:
      retry_initial_backoff_secs: 1
      retry_max_duration_secs: 3600


  clickhouse_sink:
    type: clickhouse
    inputs:
      - clickhouse_route
    endpoint: http://clickhouse:8123
    table: "{{ log_type }}"
    format: json_each_row
    skip_unknown_fields: true
    acknowledgements:
      enabled: true
    auth:
      strategy: basic
      user: default
      password: ""
    batch:
      max_events: 1
      timeout_secs: 1
    buffer:
      type: disk
      max_size: 268435490
      when_full: block
    healthcheck:
      enabled: true
    database: log_database


    
